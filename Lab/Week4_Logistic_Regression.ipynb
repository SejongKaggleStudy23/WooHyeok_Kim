{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Week4_Logistic_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "_O_nx2QMyrdU",
        "outputId": "cba8dd2f-70d5-4156-9765-487d237d4946"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 데이터 불러오기. y값은 이미 범주형으로 되어있음.\n",
        "dat_wine=pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
        "                     'wine/wine.data',header=None)\n",
        "dat_wine.head()\n",
        "dat_wine.columns = ['class label', 'alchohol', 'malic acid', 'ash', \n",
        "                    'alcalinity of ash', 'magnesium', 'total phenols', \n",
        "                    'flavanoids', 'nonflavanoid phenols', \n",
        "                    'proanthocyanins', 'color intensity', 'hue', \n",
        "                    'OD208', 'proline']  # Column names\n",
        "print('class label:', np.unique(dat_wine['class label']))  # .unique() 이용,  Class 출력\n",
        "dat_wine.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class label: [1 2 3]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class label</th>\n",
              "      <th>alchohol</th>\n",
              "      <th>malic acid</th>\n",
              "      <th>ash</th>\n",
              "      <th>alcalinity of ash</th>\n",
              "      <th>magnesium</th>\n",
              "      <th>total phenols</th>\n",
              "      <th>flavanoids</th>\n",
              "      <th>nonflavanoid phenols</th>\n",
              "      <th>proanthocyanins</th>\n",
              "      <th>color intensity</th>\n",
              "      <th>hue</th>\n",
              "      <th>OD208</th>\n",
              "      <th>proline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   class label  alchohol  malic acid  ...   hue  OD208  proline\n",
              "0            1     14.23        1.71  ...  1.04   3.92     1065\n",
              "1            1     13.20        1.78  ...  1.05   3.40     1050\n",
              "2            1     13.16        2.36  ...  1.03   3.17     1185\n",
              "3            1     14.37        1.95  ...  0.86   3.45     1480\n",
              "4            1     13.24        2.59  ...  1.04   2.93      735\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5Gm-TaGyrdX"
      },
      "source": [
        "# 전체 data를 training set과 test set으로 split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X, y = dat_wine.iloc[:,1:].values, dat_wine.iloc[:,0].values\n",
        "X_train, X_test, y_train,y_test = \\\n",
        "    train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_mq8ZQbyrdX"
      },
      "source": [
        "# 표준화\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "std = StandardScaler()\n",
        "X_train_std = std.fit_transform(X_train)\n",
        "X_test_std = std.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpJwniRuyrdY"
      },
      "source": [
        "# Logistic Regression with L2 or L1 Regularization\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr2_10 = LogisticRegression(penalty='l2', C=10.0)  # L2 with C(=1/λ)=10\n",
        "lr2_1 = LogisticRegression(penalty='l2', C=1.0)    # L2 with C(=1/λ)=1\n",
        "lr2_0_1 = LogisticRegression(penalty='l2', C=0.1)  # L2 with C(=1/λ)=0.1\n",
        "\n",
        "lr1_10 = LogisticRegression(penalty='none', C=10.0)  # L1 with C(=1/λ)=10\n",
        "lr1_1 = LogisticRegression(penalty='none', C=1.0)    # L1 with C(=1/λ)=1\n",
        "lr1_0_1 = LogisticRegression(penalty='none', C=0.1)  # L1 with C(=1/λ)=0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkPqwX_iyrdY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "630ec9ac-55d6-46a6-c912-370301ee5968"
      },
      "source": [
        "# 규제화 방법(L2 or L1)과 규제강도(λ)를 바꿔가며 accuracy score 계산\n",
        "lr2_10.fit(X_train_std, y_train)\n",
        "print('Training accuracy with L2 and λ=0.1:', lr2_10.score(X_train_std, y_train))\n",
        "print('Test accuracy with L2 and λ=0.1:', lr2_10.score(X_test_std, y_test))\n",
        "\n",
        "lr2_1.fit(X_train_std, y_train)  # warning..\n",
        "print('Training accuracy with L2 and λ=1:', lr2_1.score(X_train_std, y_train))\n",
        "print('Test accuracy with L2 and λ=1:', lr2_1.score(X_test_std, y_test))\n",
        "\n",
        "lr2_0_1.fit(X_train_std, y_train)\n",
        "print('Training accuracy with L2 and λ=10:', lr2_0_1.score(X_train_std, y_train))\n",
        "print('Test accuracy with L2 and λ=10:', lr2_0_1.score(X_test_std, y_test))\n",
        "\n",
        "lr1_10.fit(X_train_std, y_train)\n",
        "print('Training accuracy with L1 and λ=0.1:', lr1_10.score(X_train_std, y_train))\n",
        "print('Test accuracy with L1 and λ=0.1:', lr1_10.score(X_test_std, y_test))\n",
        "\n",
        "lr1_1.fit(X_train_std, y_train)\n",
        "print('Training accuracy with L1 and λ=1:', lr1_1.score(X_train_std, y_train))\n",
        "print('Test accuracy with L1 and λ=1:', lr1_1.score(X_test_std, y_test))\n",
        "\n",
        "lr1_0_1.fit(X_train_std, y_train)\n",
        "print('Training accuracy with L1 and λ=10:', lr1_0_1.score(X_train_std, y_train))\n",
        "print('Test accuracy with L1 and λ=10:', lr1_0_1.score(X_test_std, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy with L2 and λ=0.1: 1.0\n",
            "Test accuracy with L2 and λ=0.1: 0.9814814814814815\n",
            "Training accuracy with L2 and λ=1: 1.0\n",
            "Test accuracy with L2 and λ=1: 0.9814814814814815\n",
            "Training accuracy with L2 and λ=10: 1.0\n",
            "Test accuracy with L2 and λ=10: 1.0\n",
            "Training accuracy with L1 and λ=0.1: 1.0\n",
            "Test accuracy with L1 and λ=0.1: 0.9814814814814815\n",
            "Training accuracy with L1 and λ=1: 1.0\n",
            "Test accuracy with L1 and λ=1: 0.9814814814814815\n",
            "Training accuracy with L1 and λ=10: 1.0\n",
            "Test accuracy with L1 and λ=10: 0.9814814814814815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of1smq3gyrdZ",
        "outputId": "8fee0061-4690-4345-cd5f-905d2f9b52c4"
      },
      "source": [
        "# L2 규제의 규제강도(C=1/λ)를 바꿔가며 계수 추정치 관찰\n",
        "# 절편\n",
        "print(lr2_10.intercept_)\n",
        "print(lr2_1.intercept_)\n",
        "print(lr2_0_1.intercept_)\n",
        "# 가중치, 계수\n",
        "print(lr2_10.coef_)\n",
        "print(lr2_1.coef_)\n",
        "print(lr2_0_1.coef_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.95775485 -2.03400782 -3.43291968]\n",
            "[-1.21122348 -1.03039089 -1.97016349]\n",
            "[-0.60105075 -0.40659682 -0.88778119]\n",
            "[[ 2.31301856  0.42623793  0.88915733 -2.72742063  0.34471977  0.57063457\n",
            "   1.5328249  -0.05747067 -0.14460937  0.02365977 -0.1387799   1.54453754\n",
            "   2.51006889]\n",
            " [-2.4473801  -1.00394827 -2.00966039  1.87532324 -0.35077231 -1.05421584\n",
            "   1.79723557  0.73213183  0.6214013  -2.34983446  1.65715682  0.1387643\n",
            "  -3.41020463]\n",
            " [ 0.6567932   0.46268852  1.37035647  0.34767553  0.30329156  0.47280507\n",
            "  -3.25015231 -0.70851215 -0.5273707   1.92694485 -1.61435627 -1.42974821\n",
            "   0.11678842]]\n",
            "[[ 1.30549004  0.14549881  0.46036028 -1.44931922  0.10031419  0.36298361\n",
            "   0.87213708 -0.11270049  0.02097189  0.09455712  0.05891572  0.8160453\n",
            "   1.64325979]\n",
            " [-1.52090137 -0.57645877 -0.92554937  0.93057203 -0.32301482 -0.32640468\n",
            "   0.67685016  0.31393307  0.31558446 -1.26644027  0.89364797  0.11339036\n",
            "  -1.69368203]\n",
            " [ 0.36275758  0.42896676  0.49386343  0.31296385  0.264319   -0.06947203\n",
            "  -1.46434622 -0.25588726 -0.3351988   1.22872304 -0.93480599 -0.87806706\n",
            "   0.03518467]]\n",
            "[[ 0.64385668 -0.03259114  0.20017065 -0.60262888  0.08699275  0.28244452\n",
            "   0.43400895 -0.14176713  0.1027396   0.1029182   0.13003776  0.36134179\n",
            "   0.81512579]\n",
            " [-0.80668849 -0.31288479 -0.34958579  0.36577259 -0.22628528 -0.06632797\n",
            "   0.15689683  0.11568186  0.11340989 -0.65342747  0.36771523  0.14353411\n",
            "  -0.75696621]\n",
            " [ 0.20744995  0.34929972  0.14174404  0.17927779  0.12849009 -0.21097501\n",
            "  -0.56606133  0.01113167 -0.19533286  0.62182458 -0.49245313 -0.51068673\n",
            "  -0.0115737 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQhJHREUyrdZ",
        "outputId": "13f733e6-6113-48d8-848c-06e0be2da272"
      },
      "source": [
        "# L2 규제의 규제강도(C=1/λ)를 바꿔가며 계수 추정치 관찰\n",
        "# 절편\n",
        "print(lr1_10.intercept_)\n",
        "print(lr1_1.intercept_)\n",
        "print(lr1_0_1.intercept_)\n",
        "# 가중치, 계수\n",
        "print(lr1_10.coef_)\n",
        "print(lr1_1.coef_)\n",
        "print(lr1_0_1.coef_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-2.14035866 -2.65922403 -4.16476189]\n",
            "[-1.11958913 -1.13559733 -2.22622892]\n",
            "[-0.29617765 -0.08175121 -0.78575087]\n",
            "[[ 2.94105456  0.28661411  1.10478325 -3.13661938  0.244986    0.\n",
            "   1.89714188  0.          0.          0.          0.          1.8919487\n",
            "   3.25251407]\n",
            " [-2.86050792 -1.01088098 -2.63913443  1.99728475  0.         -0.94773025\n",
            "   1.78186294  0.72894868  1.12998626 -3.44013092  2.28398555  0.\n",
            "  -5.19649767]\n",
            " [ 0.90803487  0.20038068  1.91121041  0.          0.20378065  0.18872926\n",
            "  -5.50689492 -0.9494814  -0.02848173  2.70832306 -1.49302574 -0.84299314\n",
            "   0.        ]]\n",
            "[[ 1.41320331  0.          0.27838963 -1.36300459  0.          0.\n",
            "   1.27114553  0.          0.          0.          0.          0.7122173\n",
            "   2.22002694]\n",
            " [-1.68928458 -0.37315879 -0.8247154   0.68070422 -0.11987273  0.\n",
            "   0.3959537   0.01144807  0.24951005 -1.42631508  1.05996578  0.\n",
            "  -2.23082137]\n",
            " [ 0.          0.23194873  0.50743267  0.          0.16131275  0.\n",
            "  -2.48984944 -0.11935498  0.          1.65069688 -0.8879895  -0.37586666\n",
            "   0.        ]]\n",
            "[[ 0.26740112  0.          0.         -0.19506386  0.          0.\n",
            "   0.68721818  0.          0.          0.          0.          0.\n",
            "   1.30347596]\n",
            " [-0.8996666   0.          0.          0.          0.          0.\n",
            "   0.          0.          0.         -0.84743703  0.          0.\n",
            "  -0.43541368]\n",
            " [ 0.          0.1032016   0.          0.          0.          0.\n",
            "  -0.73685702  0.          0.          0.59848813 -0.31616716 -0.35285035\n",
            "   0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihCONnUzyrdZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ce3f762-0d67-4029-9d37-40dfc4a0b50a"
      },
      "source": [
        "lr2_10.fit(X_train, y_train)\n",
        "print('Training accuracy with L2 and λ=0.1:', lr2_10.score(X_train, y_train))\n",
        "print('Test accuracy with L2 and λ=0.1:', lr2_10.score(X_test, y_test))\n",
        "\n",
        "lr2_1.fit(X_train, y_train)  # warning..\n",
        "print('Training accuracy with L2 and λ=1:', lr2_1.score(X_train, y_train))\n",
        "print('Test accuracy with L2 and λ=1:', lr2_1.score(X_test, y_test))\n",
        "\n",
        "lr2_0_1.fit(X_train, y_train)\n",
        "print('Training accuracy with L2 and λ=10:', lr2_0_1.score(X_train, y_train))\n",
        "print('Test accuracy with L2 and λ=10:', lr2_0_1.score(X_test, y_test))\n",
        "\n",
        "lr1_10.fit(X_train, y_train)\n",
        "print('Training accuracy with L1 and λ=0.1:', lr1_10.score(X_train, y_train))\n",
        "print('Test accuracy with L1 and λ=0.1:', lr1_10.score(X_test, y_test))\n",
        "\n",
        "lr1_1.fit(X_train, y_train)\n",
        "print('Training accuracy with L1 and λ=1:', lr1_1.score(X_train, y_train))\n",
        "print('Test accuracy with L1 and λ=1:', lr1_1.score(X_test, y_test))\n",
        "\n",
        "lr1_0_1.fit(X_train, y_train)\n",
        "print('Training accuracy with L1 and λ=10:', lr1_0_1.score(X_train, y_train))\n",
        "print('Test accuracy with L1 and λ=10:', lr1_0_1.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training accuracy with L2 and λ=0.1: 0.9838709677419355\n",
            "Test accuracy with L2 and λ=0.1: 0.9074074074074074\n",
            "Training accuracy with L2 and λ=1: 0.9758064516129032\n",
            "Test accuracy with L2 and λ=1: 0.9259259259259259\n",
            "Training accuracy with L2 and λ=10: 0.9758064516129032\n",
            "Test accuracy with L2 and λ=10: 0.9074074074074074\n",
            "Training accuracy with L1 and λ=0.1: 0.967741935483871\n",
            "Test accuracy with L1 and λ=0.1: 0.9074074074074074\n",
            "Training accuracy with L1 and λ=1: 0.967741935483871\n",
            "Test accuracy with L1 and λ=1: 0.9074074074074074\n",
            "Training accuracy with L1 and λ=10: 0.967741935483871\n",
            "Test accuracy with L1 and λ=10: 0.9074074074074074\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpDJIzmZyrda",
        "outputId": "f1b70a3f-ee68-4c7d-b864-876fc4fd3573"
      },
      "source": [
        "print(lr2_10.intercept_)\n",
        "print(lr2_1.intercept_)\n",
        "print(lr2_0_1.intercept_)\n",
        "\n",
        "print(lr2_10.coef_)\n",
        "print(lr2_1.coef_)\n",
        "print(lr2_0_1.coef_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.96456004  1.34118396 -0.22390307]\n",
            "[-0.16655119  0.26945958 -0.07022366]\n",
            "[-0.02988443  0.05684191 -0.02613036]\n",
            "[[-8.35654939e-01  1.44150350e+00  1.46570959e+00 -1.27574191e+00\n",
            "   2.09578947e-02  9.54417242e-01  3.00222626e+00 -7.24568499e-02\n",
            "  -1.22897003e+00 -5.54461321e-01 -1.84306873e-02  6.28614504e-01\n",
            "   2.67981633e-02]\n",
            " [ 1.09359574e+00 -2.12095717e+00 -1.21322442e+00  7.34605497e-01\n",
            "  -1.75690138e-02 -1.04233634e+00  1.23716271e+00  2.85603369e+00\n",
            "   1.60402367e+00 -2.48226700e+00  3.26535993e+00 -6.66781906e-01\n",
            "  -2.21810800e-02]\n",
            " [-7.02348476e-01  1.19098753e+00  5.88127902e-01  4.03900064e-02\n",
            "   7.27945504e-02 -3.70618302e-01 -3.76294649e+00 -6.02758148e-01\n",
            "  -7.46352348e-01  1.33638830e+00 -1.36735959e+00 -2.11442785e+00\n",
            "   2.97823296e-03]]\n",
            "[[-3.58983950e-01  4.10637468e-01  3.53872093e-01 -7.42979348e-01\n",
            "  -3.36933385e-02  5.44683231e-01  1.12999767e+00 -5.75279878e-02\n",
            "   5.03329511e-02 -1.89873020e-01  7.55818484e-03  6.14639419e-01\n",
            "   2.00108374e-02]\n",
            " [ 5.83708283e-01 -1.21331455e+00 -2.03985537e-01  4.08802542e-01\n",
            "   1.60762850e-02 -1.92359585e-01  5.48138539e-01  4.11511208e-01\n",
            "   5.57258377e-01 -1.55212916e+00  6.23184914e-01  9.32319399e-02\n",
            "  -1.48922934e-02]\n",
            " [-3.97544028e-01  6.57589662e-01 -2.75435431e-02  9.79082436e-02\n",
            "   3.00861950e-02 -6.18472516e-01 -1.73526232e+00 -6.31254597e-02\n",
            "  -5.97541591e-01  9.57138429e-01 -4.07581305e-01 -1.11166273e+00\n",
            "   5.88167601e-04]]\n",
            "[[-1.26226337e-01  3.93330206e-02  5.54428787e-02 -5.04524375e-01\n",
            "  -4.18202359e-02  1.95902320e-01  3.81751039e-01 -3.25485946e-02\n",
            "   1.18294200e-01 -1.28617540e-01  1.12477626e-02  2.39734903e-01\n",
            "   1.69833587e-02]\n",
            " [ 1.64214209e-01 -5.08066956e-01 -2.91218095e-02  2.31991185e-01\n",
            "   2.99136925e-02  7.95773000e-02  2.36972190e-01  4.87189589e-02\n",
            "   1.58168713e-01 -8.00859234e-01  1.53495930e-01  2.36565769e-01\n",
            "  -1.01836247e-02]\n",
            " [-1.38084802e-01  3.75170958e-01 -2.17741103e-02  5.43678605e-02\n",
            "  -8.24859327e-05 -2.96933155e-01 -7.25585571e-01 -2.17865191e-04\n",
            "  -2.74424219e-01  6.64387737e-01 -1.59728574e-01 -5.06679873e-01\n",
            "  -1.48772700e-03]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkR1okpUyrda",
        "outputId": "2d394a0e-eb91-4d19-ffec-9112bae482a4"
      },
      "source": [
        "print(lr1_10.intercept_)\n",
        "print(lr1_1.intercept_)\n",
        "print(lr1_0_1.intercept_)\n",
        "\n",
        "print(lr1_10.coef_)\n",
        "print(lr1_1.coef_)\n",
        "print(lr1_0_1.coef_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0.]\n",
            "[0. 0. 0.]\n",
            "[0. 0. 0.]\n",
            "[[-1.01302850e+00  2.39608807e+00  1.71991521e-01 -1.86127869e+00\n",
            "   8.34950377e-02  0.00000000e+00  7.14119679e+00  0.00000000e+00\n",
            "  -2.97774639e+00 -8.18214103e-01  0.00000000e+00  0.00000000e+00\n",
            "   3.49411930e-02]\n",
            " [ 1.31679390e+00 -2.73038551e+00 -3.38001837e+00  9.41289186e-01\n",
            "   0.00000000e+00 -1.95012089e+00  1.91790955e+00  1.23238533e+01\n",
            "   2.58858839e+00 -3.24670367e+00  4.31646082e+00 -5.15270908e-01\n",
            "  -2.73681581e-02]\n",
            " [-2.78642772e-01  1.40323545e+00  0.00000000e+00  3.31841490e-02\n",
            "   1.17547347e-01  0.00000000e+00 -7.98095839e+00  0.00000000e+00\n",
            "   0.00000000e+00  1.24944131e+00 -2.20232349e+00 -3.76082391e+00\n",
            "  -6.33896232e-04]]\n",
            "[[-2.34793075e-02  8.11723037e-02  0.00000000e+00 -7.05407269e-01\n",
            "  -4.59876204e-02  0.00000000e+00  1.97035783e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   1.76174386e-02]\n",
            " [ 5.87514555e-01 -1.21890126e+00  0.00000000e+00  4.28854104e-01\n",
            "   2.44013384e-02  0.00000000e+00  5.27866494e-01  0.00000000e+00\n",
            "   1.20970058e-01 -1.70853789e+00  0.00000000e+00  0.00000000e+00\n",
            "  -1.43148923e-02]\n",
            " [-1.76756067e-01  4.52854263e-01  0.00000000e+00  1.66366583e-02\n",
            "   1.78552369e-02  0.00000000e+00 -3.17827470e+00  0.00000000e+00\n",
            "   0.00000000e+00  9.13911761e-01  0.00000000e+00 -1.00770209e+00\n",
            "   4.42623020e-04]]\n",
            "[[ 0.          0.          0.         -0.47505119 -0.03844061  0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.01561705]\n",
            " [ 0.         -0.05578404  0.          0.18704578  0.05098921  0.\n",
            "   0.          0.          0.         -0.85684899  0.          0.\n",
            "  -0.00782387]\n",
            " [ 0.          0.          0.          0.         -0.00911855  0.\n",
            "  -1.12960892  0.          0.          0.69050998  0.          0.\n",
            "  -0.00247412]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}